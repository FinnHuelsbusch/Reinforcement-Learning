# Task 1 

Theoretical task . Target and behavior policy. 

## a 

E depends on the behavior policy while the value function depends on the final policy

## b 



## c 

Nur zeigen f√ºr eine finite Anzahl an steps 





If epsilon is small we do not explore enough for a good learning rate. If it is to small we would not explore to much and end in a hole to often. So we can init our q values as we want so make use of that. Init it with high q values. THis will lead a greedy agent to explore this state. 



If we come up with other ideas to make it faster. Maybe use another wrapper from gym ;-) 